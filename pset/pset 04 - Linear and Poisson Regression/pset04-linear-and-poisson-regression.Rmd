---
title: "PSET 04 - Linear regression, Bootstrapping"
author: "S&DS 361"
date: "2024-02-21"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Part 1: Data acquisition and cleaning

#### 1. Add comments to `get.flight.location.data.r`

The R script `get.flight.location.data.r` contains code that scrapes flight location data from ADSBexchange.com. It is pasted below. The script contains no comments (except at the very top of the script). Please add comments to the code below everywhere there is a `##` explaining what that chunk of code does. 

You don't need to write paragraphs, just a line or two for each one. Many of the functions below will look familiar, but some may be new to you and will require you to check R help or the interwebs. Feel free to run the code (by, for example, copying it into it's own R script and running line by line) to check what the outputs are before writing your comment. 


```
## get.flight.location.data.r
## scrape data from adsbexchange
## Got URL from the May example under readsb-hist that was give here
## https://www.adsbexchange.com/data-samples/
library(tidyverse)
library(jsonlite)

##
du = expand.grid(sec=seq(0,55, by=5), 
                 min=0:59, 
                 hr=0:23)

##
du = du %>% 
  select(hr, min, sec) 
head(du)

##
du = du %>% 
  mutate(hr  = str_pad(hr , 2, pad='0'), 
         min = str_pad(min, 2, pad='0'), 
         sec = str_pad(sec, 2, pad='0'))

##
du = du %>% 
  mutate(url = paste0('https://samples.adsbexchange.com/readsb-hist/2022/12/01/', 
                      hr, min, sec, 
                      'Z.json.gz'))
head(du)                

##
du = du %>%
  filter(as.numeric(min) %% 10 ==0, 
         sec=='00')

##
for(j in 1:nrow(du)){
  
  cat(j, '')
  
  ##
  temp.url = du$url[j]
  temp.url = url(temp.url)
  con = gzcon(temp.url)
  d = fromJSON(con)
  
  ##
  d = d$aircraft
  
  ##
  filename = gsub('.+/', '', du$url[j])
  filename = gsub('[.].+', '', filename)
  filename = paste0('data/flight-locations/', filename, '.rds')
  saveRDS(d, file=filename)
  
}
```


#### 2. Add comments to `clean.flight.location.data.r`

The R script `clean.flight.location.data.r` contains code that clean the data obtained by `get.flight.location.data.r`. The script is pasted below. It also contains no comments. Please add comments everywhere there is a `##` explaining what that chunk of code does. 

You don't need to write paragraphs, just a line or two for each one. Many of the functions below will look familiar, but some may be new to you and will require you to check R help or the interwebs. Feel free to run the code (by, for example, copying it into it's own R script) to check what the outputs are before writing your comment.

```
## clean.flight.location.data.r
## clean the flight location data obtained by using get.flight.location.data.r

## 
filenames = dir('data/flight-locations/', full.names = T)
head(filenames)

##
df = NULL

## 
for(filename in filenames){
  
  ##
  time = gsub('.+[/]|Z.+', '', filename)
  hr = substr(time, 1, 2)
  min= substr(time, 3, 4)
  
  ##
  time = paste0(hr, ":", min)
  hr = as.numeric(hr)
  min= as.numeric(min)
  
  cat(time, '')
  
  d = readRDS(filename)
  
  ##
  dd = d %>%
    
    ##
    unnest(lastPosition, names_sep = '.') %>% 
    
    mutate(
      
      ##
      flight = trimws(flight), 
      
      ##
      lat = ifelse(!is.na(lat), lat, rr_lat),
      lon = ifelse(!is.na(lon), lon, rr_lon),
      lat = ifelse(is.na(lat) & lastPosition.seen_pos<=600, lastPosition.lat, lat),
      lon = ifelse(is.na(lon) & lastPosition.seen_pos<=600, lastPosition.lon, lon),
      
      ## 
      alt_baro = ifelse(alt_baro=='ground', 0       , alt_baro),
      alt_baro = as.numeric(alt_baro),
      alt      = ifelse(!is.na(alt_geom)  , alt_geom, alt_baro),
      
      ##
      speed   = ifelse(!is.na(gs), gs, tas),
      
      ##
      heading = case_when(
        !is.na(track)                                             ~ track,
        is.na(track) & !is.na(true_heading)                       ~ true_heading,
        is.na(track) &  is.na(true_heading) & !is.na(nav_heading) ~ nav_heading, 
        TRUE ~ track), 
      
      ##
      hr  =  hr, 
      min = min, 
      time = time) %>%
    
    ##
    select(hex, type, flight, lat, lon, alt, speed, heading, hr, min, time) %>%
    
    ##
    filter(alt!=0)
  
  ##
  df = rbind(df, dd)
  
}

saveRDS(df, file='data/flight.locations.rds')

```

## Part 2: Linear regression

#### 3. Different codings for categorical variables

Suppose you are predicting `log(value)` in our `brandford.csv` data set. Suppose you build one model with `log(living)`, `grade`, and `style` as predictors, and for a second model you use `relevel` to choose `Mobile Home` as the reference level for the `style` variable, instead of using the default reference `Bungalow` that `lm` uses in the first model. Show that 

- the predictions obtained from the two models are the same
- the CIs and PIs are the same
- adjusted R^2 is the same
- the intercept differs by a constant
- the coefficients for `style` all differ by the same constant 
- the coefficients of `Mobile Home` in model 1 and `Bungalow` in model 2 are that same constant or -1*constant. 
- all other coefficients (`log(living)` and `grade`) and standard errors are the same (up to 13 decimal places)


#### 4. Simulation

Suppose $y$ has a linear, non-deterministic relationship with $x$ that can be represented by the model $y = 1 + 2x + \epsilon$, where $\epsilon \sim N(0,1)$. Simulate some data by doing the following: 

a. Create a random sample of 10,000 $x$ values on $[0,1]$.
b. Create a random sample of 10,000 $\epsilon$ values from $N(0,1)$.
c. Create a random sample of 10,000 $y$ values using $y = 1 + 2x + \epsilon$.

Then, 

d. Fit a simple linear regression model to the data and report the estimated coefficients and estimated model standard deviation. Are the estimates what you would expect? Why or why not? 

e. Plot the scatter plot of $y$ versus $x$, and add a regression line to the plot. Does this visualization look as you would have expected? Do the linear regression assumptions appear to hold? Why or why not? 


## Part 3: Poisson Regression

### ChatGPT
#### 5. After asking ChatGPT "What is Poisson Regression?" I then asked "why couldn't I use linear regression instead?" Part of ChatGPT's response is below.  For each bullet point, name at least one part of ChatGPT's response that isn't *quite* accurate. 

- "Linear regression assumes that the response variable is continuous and normally distributed, and it is not suitable for modeling count data that is typically discrete and non-negative"
- "Furthermore, Poisson regression models the natural logarithm of the mean of the response variable as a linear function of the predictor variables, which can handle situations where the mean and variance of the response variable are not equal."


### Tesla's Safety Score

Tesla uses data collected on each driver (using their cameras and possibly other sensors) to create a Safety Score for that driver. According to [Tesla's website](https://www.tesla.com/support/safety-score#version-2.0), "The Safety Score (Beta) is an assessment of your driving behavior based on several metrics called Safety Factors. These are combined to estimate the likelihood that your driving could result in a future collision... The Safety Score (Beta) is intended to provide drivers transparency and feedback of their driving behaviors to encourage safer driving and potentially pay less for their insurance. The Safety Score is a value between 0 and 100, where a higher score indicates safer driving. Most drivers are expected to have a Safety Score of 80 or above."

Tesla publishes the formulas they use to compute the Safety Score. "In order to calculate your daily Safety Score, we use the Predicted Collision Frequency (PCF) formula below to predict how many collisions may occur per 1 million miles driven, based on your driving behaviors measured by your Tesla vehicle."

$$ \textrm{Predicted Collision Frequency (PCF)} = 0.83220180 
\times 1.012555104^{\textrm{Forward Collision Warnings per 1,000 Non-Autopilot Miles}} \\
\times 1.16460827^{\textrm{Hard Braking}} \\
\times 1.01498152^{\textrm{Aggressive Turning}} \\
\times 1.00245084^{\textrm{Unsafe Following Time}} \\
\times 1.40663310^{\textrm{Forced Autopilot Disengagement}} \\
\times 1.05018975^{\textrm{Late Night Driving}} \\
\times 1.00939791^{\textrm{Excessive Speeding}} \\
\times 1.00901189^{\textrm{Unbuckled Driving}}
$$

The Safety Factors like "Hard Breaking" are described on the page linked above. They convert the PCF to the Safety Score using 

$$ \textrm{Safety Score} = 112.29263237 - 14.77121589 \cdot \textrm{PCF} $$

#### 6. The PCF formula looks like it could be the result of fitting a Poisson regression model to their Safety Factors and collisions data. Discuss why a Poisson Regression model would or would not be a reasonable model for this data. 



#### 7. Let's assume the PCF formula did in fact come from a Poisson Regression model. What was $\beta_{\textrm{Hard Breaking}}$, the coefficient for Hard Breaking, in that model? 



#### 8. What is the interpretation of the term $1.10385444^{\textrm{Hard Braking}}$ in the PCF formula? 



### EV charging stations

The following data set is census data set from earlier in the course with new columns added that have the number of Level 2 (`lev2`) and Level 3 (`lev3`) electric vehicle charging stations for each census tract (`GEOID`). 

```{r}
dc = readRDS('data/tracts.and.census.with.EV.stations.rds')
dc = dc@data ## keep just the data frame, not the polygons
dc = dc %>% 
  
  ## Change NAs to 0s, and 
  ## since charging stations seem to come in pairs, create new columns
  ## for pairs of charging stations
  mutate(lev2 = ifelse(is.na(lev2), 0, lev2), 
         lev3 = ifelse(is.na(lev3), 0, lev3), 
         lev2pairs = round(lev2/2),
         lev3pairs = round(lev3/2)) %>%
  
  ## Keep tracts with at least one charging station, 
  ## and get rid of a couple of outliers
  filter((lev2!=0 | lev3!=0) & lev2 <= 50) 


```

#### 9. Suppose we want to quickly determine if the number of Level 2 charging stations in a census track looks roughly Poisson. (For this problem, consider all the data at once, not just the data for smaller subsets of $x$.) Let's compare the data to a Poisson distribution with the same mean as the data.  Plot the actual distribution of the data along with this Poisson distribution in a way that allows for a quick comparison of the two. Does the number pairs of Level 2 charging stations in a census tract look Poisson distributed?


#### 10. Fit a Poisson Regression model for predicting Level 2 charging stations in a census tract using `house.value` and `age`. How does this model compare to the null model? Considering your observations in #9, what outputs of this model are reasonable/unreasonable? 


