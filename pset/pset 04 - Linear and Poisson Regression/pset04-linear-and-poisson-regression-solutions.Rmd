---
title: "PSET 04 - Linear regression, Poisson Regression"
author: "S&DS 361"
date: "2024-02-21"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(pubtheme)
```

## Part 1: Data acquisition and cleaning

#### 1. Add comments to `get.flight.location.data.r`

The R script `get.flight.location.data.r` contains code that scrapes flight location data from ADSBexchange.com. It is pasted below. The script contains no comments (except at the very top of the script). Please add comments to the code below everywhere there is a `##` explaining what that chunk of code does. 

You don't need to write paragraphs, just a line or two for each one. Many of the functions below will look familiar, but some may be new to you and will require you to check R help or the interwebs. Feel free to run the code (by, for example, copying it into it's own R script and running line by line) to check what the outputs are before writing your comment. You'll need to create a folder called `data/flight-locations` in order for the last line of the code to run. I suggest changing the for loop to say `j in 1:2` instead of `j in 1:nrow(du)` so that you can test the code without having to wait for all the data to download.


```
## get.flight.location.data.r
## scrape data from adsbexchange
## Got URL from the May example under readsb-hist that was give here
## https://www.adsbexchange.com/data-samples/)
library(tidyverse)
library(jsonlite)

## Create from 00:00:00 to 23:59:55, in increments of 5 seconds
du = expand.grid(sec=seq(0,55, by=5), 
                 min=0:59, 
                 hr=0:23)

## Reorder the columns
du = du %>% 
  select(hr, min, sec) 
head(du)

## Make each column two digits 
## by padding with zeros on the left if necessary
du = du %>% 
  mutate(hr  = str_pad(hr , 2, pad='0'), 
         min = str_pad(min, 2, pad='0'), 
         sec = str_pad(sec, 2, pad='0'))

## Create a column of URLs to look up using the hr, min, sec
du = du %>% 
  mutate(url = paste0('https://samples.adsbexchange.com/readsb-hist/2022/12/01/', 
                      hr, min, sec, 
                      'Z.json.gz'))
head(du)                

## Hmm, on second thought, since there is a ridiculous amount of data, 
## let's do every 10 minutes to start. 
## We can easily change this later.
du = du %>%
  filter(as.numeric(min) %% 10 ==0, 
         sec=='00')

## Loop over the rows of `du`, 
## scrape data from the URL
## Convert it to a data frame
## and save it
for(j in 1:nrow(du)){
  
  cat(j, '')
  
  ## Find the URL for this row, create a connection, and 
  ## pull the JSON file into a list
  temp.url = du$url[j]
  temp.url = url(temp.url)
  con = gzcon(temp.url)
  d = fromJSON(con)
  
  ## Extract the data frame we are interested in from that list
  d = d$aircraft
  
  ## Create a file name with the time, then save
  filename = gsub('.+/', '', du$url[j])
  filename = gsub('[.].+', '', filename)
  filename = paste0('data/flight-locations/', filename, '.rds')
  saveRDS(d, file=filename)
  
}
```


#### 2. Add comments to `clean.flight.location.data.r`

The R script `clean.flight.location.data.r` contains code that clean the data obtained by `get.flight.location.data.r`. The script is pasted below. It also contains no comments. Please add comments everywhere there is a `##` explaining what that chunk of code does. 

You don't need to write paragraphs, just a line or two for each one. Many of the functions below will look familiar, but some may be new to you and will require you to check R help or the interwebs. Feel free to run the code (by, for example, copying it into it's own R script) to check what the outputs are before writing your comment.

```
## clean.flight.location.data.r
## clean the flight location data obtained by using get.flight.location.data.r

## Find all files in data/flight-locations. 
## Give full (relative) path with the file names
filenames = dir('data/flight-locations/', full.names = T)
head(filenames)

## Create an empty data frame that we will add to later
df = NULL

## Loop over files, read them in, clean them, and add them to df
for(filename in filenames){
  
  ## Extract the hour and minute
  ## The . is a wildcard. 
  ## The + means match the previous thing one or more times
  ## So .+ means match anything one or more times.
  time = gsub('.+[/]|Z.+', '', filename)
  hr = substr(time, 1, 2)
  min= substr(time, 3, 4)
  
  ## Create a nicely formatted time. Print the time to show progress
  time = paste0(hr, ":", min)
  hr = as.numeric(hr)
  min= as.numeric(min)
  
  cat(time, '')
  
  d = readRDS(filename)

  ## Create a new data frame, add edit columns of d
  dd = d %>%
    
    ## The column lastPosition is itself a data.frame
    ## so unnest this so the columns in lastPosition
    ## appear as columns in dd
    unnest(lastPosition, names_sep = '.') %>% 
    
    mutate(
      
      ## Get ride of leading and trailing whitespace in the flight column
      flight = trimws(flight), 
      
      ## If lat exists, use it. If lat doesn't exist, use rr_lat. 
      ## If lat still doesn't exist, and 
      ## lastPosition is less than 10 minutes, use lastPosition.lat
      ## Likewise for lon
      lat = ifelse(!is.na(lat), lat, rr_lat),
      lon = ifelse(!is.na(lon), lon, rr_lon),
      lat = ifelse(is.na(lat) & lastPosition.seen_pos<=600, lastPosition.lat, lat),
      lon = ifelse(is.na(lon) & lastPosition.seen_pos<=600, lastPosition.lon, lon),
      
      ## Convert alt_baro to numeric
      ## If alt_baro doesn't exist, use alt_geom
      alt_baro = ifelse(alt_baro=='ground', 0       , alt_baro),
      alt_baro = as.numeric(alt_baro),
      alt      = ifelse(!is.na(alt_geom)  , alt_geom, alt_baro),
      
      ## Use gs for speed. If gs doesn't exist, use tas
      speed   = ifelse(!is.na(gs), gs, tas),
      
      ## For heading, use track.
      ## If track doesn't exist, but true_heading does, use true_heading
      ## If track and true_heading don't exist, but nav_heading does, use that.
      ## If none of those apply, use track (keep as NA)
      heading = case_when(
        !is.na(track)                                             ~ track,
        is.na(track) & !is.na(true_heading)                       ~ true_heading,
        is.na(track) &  is.na(true_heading) & !is.na(nav_heading) ~ nav_heading, 
        TRUE ~ track), 
      
      ## Create columns for hr, min, and time in the data.frame
      ## Needed to unique identify these observations, 
      ## since eventually we are adding this to `df` 
      hr  =  hr, 
      min = min, 
      time = time) %>%
    
    ## Keep the columns we want, put them in the order we want
    select(hex, type, flight, lat, lon, alt, speed, heading, hr, min, time) %>%
    
    ## Get rid of aircraft that are on the ground. 
    filter(alt!=0)
  
  ## Add this data to the bottom of `df`
  df = rbind(df, dd)
  
}

saveRDS(df, file='data/flight.locations.rds')

```

## Part 2: Linear regression

#### 3. Different codings for categorical variables

Suppose you are predicting `log(value)` in our `brandford.csv` data set. Suppose you build one model with `log(living)`, `grade`, and `style` as predictors, and for a second model you use `relevel` to choose `Mobile Home` as the reference level for the `style` variable, instead of using the default reference `Bungalow` that `lm` uses in the first model. Show that 

- the predictions obtained from the two models are the same
- the CIs and PIs are the same
- adjusted R^2 is the same
- the intercept differs by a constant
- the coefficients for `style` all differ by the same constant 
- the coefficients of `Mobile Home` in model 1 and `Bungalow` in model 2 are that same constant or -1*constant. 
- all other coefficients (`log(living)` and `grade`) and standard errors are the same (up to 13 decimal places)

```{r}
b = read.csv('data/branford.csv')
b$style = factor(b$style)
b$style2 = relevel(b$style, 'Mobile Home')
lm1 = lm(log(value) ~ log(living) + grade + style , data=b)
lm2 = lm(log(value) ~ log(living) + grade + style2, data=b)

b$mu.hat1 = predict(lm1, newdata = b)
b$mu.hat2 = predict(lm2, newdata = b)
b$mu.hat.lower1 = predict(lm1, newdata = b, interval = 'confidence')[,"lwr"]
b$mu.hat.lower2 = predict(lm2, newdata = b, interval = 'confidence')[,"lwr"]
b$mu.hat.upper1 = predict(lm1, newdata = b, interval = 'confidence')[,"upr"]
b$mu.hat.upper2 = predict(lm2, newdata = b, interval = 'confidence')[,"upr"]
b$y.lower1 = predict(lm1, newdata = b, interval = 'confidence')[,"lwr"]
b$y.lower2 = predict(lm2, newdata = b, interval = 'confidence')[,"lwr"]
b$y.upper1 = predict(lm1, newdata = b, interval = 'confidence')[,"upr"]
b$y.upper2 = predict(lm2, newdata = b, interval = 'confidence')[,"upr"]
b %>% 
  select(mu.hat1, mu.hat2, 
         mu.hat.lower1, mu.hat.lower2, 
         mu.hat.upper1, mu.hat.upper2, 
         y.lower1, y.lower2, 
         y.upper1, y.upper2) %>%
  head()
```

```{r}
df1 = data.frame(lm1 = summary(lm1)$coef[,1:2]) %>% rownames_to_column(var = 'predictor') 
df2 = data.frame(lm2 = summary(lm2)$coef[,1:2]) %>% rownames_to_column(var = 'predictor') %>%
  mutate(predictor = gsub('style2', 'style', predictor))

df = df1 %>%
  full_join(df2, 
            by = 'predictor') %>%
  mutate(diff    = round(lm1.Estimate   - lm2.Estimate, 13),
         diff.se = round(lm1.Std..Error - lm2.Std..Error, 13)) 

df
```


```{r}
summary(lm1)$adj.r.squared
summary(lm2)$adj.r.squared
```


#### 4. Simulation

Suppose $y$ has a linear, non-deterministic relationship with $x$ that can be represented by the model $y = 1 + 2x + \epsilon$, where $\epsilon \sim N(0,1)$. Simulate some data by doing the following: 

a. Create a random sample of 10,000 $x$ values on $[0,1]$.
b. Create a random sample of 10,000 $\epsilon$ values from $N(0,1)$.
c. Create a random sample of 10,000 $y$ values using $y = 1 + 2x + \epsilon$.

Then, 

d. Fit a simple linear regression model to the data and report the estimated coefficients and estimated model standard deviation. Are the estimates what you would expect? Why or why not? 

e. Plot the scatter plot of $y$ versus $x$, and add a regression line to the plot. Does this visualization look as you would have expected? Do the linear regression assumptions appear to hold? Why or why not? 

```{r}
x = runif(10000, 0, 1)
epsilon = rnorm(10000, 0, 1)
y = 1 + 2*x + epsilon
df = data.frame(x, y)
lm1 = lm(y ~ x, data = df)
summary(lm1)
g = ggplot(df, aes(x, y)) + 
  geom_point() + 
  geom_smooth(method = 'lm', 
              color = pubred)
g %>% pub()

```


## Part 3: Poisson Regression

### ChatGPT
#### 5. After asking ChatGPT "What is Poisson Regression?" I then asked "why couldn't I use linear regression instead?" Part of ChatGPT's response is below.  For each bullet point, name at least one part of ChatGPT's response that isn't *quite* accurate. 

- "Linear regression assumes that the response variable is continuous and normally distributed, and it is not suitable for modeling count data that is typically discrete and non-negative"
- "Furthermore, Poisson regression models the natural logarithm of the mean of the response variable as a linear function of the predictor variables, which can handle situations where the mean and variance of the response variable are not equal."

For the second bullet point, Poisson regression assumes the mean and variance of the response *are* equal. Quasi-Poisson regression can handle situations where the mean is not equal to the variance, but I asked ChatGPT about (ordinary, non-quasi) Poisson regression. In textbooks, Wikipedia, and online resources, Quasi-Poisson regression is often discussed immediately after Poisson regression, so it makes sense that ChatGPT might have picked up that statement. 


For the first bullet point, the response is normally distributed *for a given x*, or for a given set of $p$ predictors $x_1, \ldots, x_p$. The response may not be normally distributed itself (although it often is). Here is an example where $y$ is defined to be $x$ plus standard normal error. Since $x$ is skewed, $y$ looks skewed, even though for a given $x$, $y$ is defined to be normally distributed. 

```{r}
n = 10000
x = rexp(n=n, rate=.5)
error = rnorm(n, mean=0, sd=1)
y = x + error
df = data.frame(x=x, y=y)

ggplot(df, aes(x = x)) +
  geom_histogram(fill  = pubred, 
                 color = pubbackgray, 
                 boundary = 0) + 
  labs(title = "Distribution of x" ,
       x = 'x', 
       y = 'Count')+  
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0), 
                     labels=comma)+
  theme_pub(type = 'hist', 
            base_size = 36/3) 


```

```{r}
ggplot(df, aes(x = y)) +
  geom_histogram(fill  = pubred, 
                 color = pubbackgray, 
                 boundary = 0) + 
  labs(title = "Distribution of y" ,
       x = 'y', 
       y = 'Count')+  
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0), 
                     labels=comma)+
  theme_pub(type = 'hist', 
            base_size = 36/3) 


```

If we build a linear regression model, the residuals *are* normally distributed, since for a given $x$, $y$ is normally distributed.

```{r}
m1 = lm(y ~ x, data=df)
df$res = m1$residuals

ggplot(df, aes(x = res)) +
  geom_histogram(fill  = pubred, 
                 color = pubbackgray, 
                 boundary = 0) + 
  labs(title = "Distribution of Residuals" ,
       x = 'Residuals', 
       y = 'Count')+  
  scale_x_continuous(expand = c(0,0), 
                     limits = c(-4,4))+
  scale_y_continuous(expand = c(0,0), 
                     labels=comma)+
  theme_pub(type = 'hist', 
            base_size = 36/3) 

```

A more obvious example is when $x$ is binary. 

```{r}
n = 10000
x = rbinom(n=n, size=1, prob=0.5)
error = rnorm(n, mean=0, sd=.25)
y = x + error
df = data.frame(x=x, y=y)

ggplot(df, aes(x = x)) +
  geom_histogram(fill  = pubred, 
                 color = pubbackgray, 
                 boundary = 0) + 
  labs(title = "Distribution of x" ,
       x = 'x', 
       y = 'Count')+  
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0), 
                     labels=comma)+
  theme_pub(type = 'hist', 
            base_size = 36/3) 

```

In this example $y$ is very far from being normal. 

```{r}
ggplot(df, aes(x = y)) +
  geom_histogram(fill  = pubred, 
                 color = pubbackgray, 
                 boundary = 0) + 
  labs(title = "Distribution of y" ,
       x = 'y', 
       y = 'Count')+  
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0), 
                     labels=comma)+
  theme_pub(type = 'hist', 
            base_size = 36/3) 


```

If we build a linear regression model, the residuals *are* normally distributed, since for a given $x$, $y$ is normally distributed.

```{r}
m1 = lm(y ~ x, data=df)
df$res = m1$residuals

ggplot(df, aes(x = res)) +
  geom_histogram(fill  = pubred, 
                 color = pubbackgray, 
                 boundary = 0) + 
  labs(title = "Distribution of Residuals" ,
       x = 'Residuals', 
       y = 'Count')+  
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0), 
                     labels=comma)+
  theme_pub(type = 'hist', 
            base_size = 36/3) 
```

### Tesla's Safety Score

Tesla uses data collected on each driver (using their cameras and possibly other sensors) to create a Safety Score for that driver. According to [Tesla's website](https://www.tesla.com/support/safety-score#version-2.0), "The Safety Score (Beta) is an assessment of your driving behavior based on several metrics called Safety Factors. These are combined to estimate the likelihood that your driving could result in a future collision... The Safety Score (Beta) is intended to provide drivers transparency and feedback of their driving behaviors to encourage safer driving and potentially pay less for their insurance. The Safety Score is a value between 0 and 100, where a higher score indicates safer driving. Most drivers are expected to have a Safety Score of 80 or above."

Tesla publishes the formulas they use to compute the Safety Score. "In order to calculate your daily Safety Score, we use the Predicted Collision Frequency (PCF) formula below to predict how many collisions may occur per 1 million miles driven, based on your driving behaviors measured by your Tesla vehicle."

$$ \textrm{Predicted Collision Frequency (PCF)} = 0.83220180 
\times 1.012555104^{\textrm{Forward Collision Warnings per 1,000 Non-Autopilot Miles}} \\
\times 1.16460827^{\textrm{Hard Braking}} \\
\times 1.01498152^{\textrm{Aggressive Turning}} \\
\times 1.00245084^{\textrm{Unsafe Following Time}} \\
\times 1.40663310^{\textrm{Forced Autopilot Disengagement}} \\
\times 1.05018975^{\textrm{Late Night Driving}} \\
\times 1.00939791^{\textrm{Excessive Speeding}} \\
\times 1.00901189^{\textrm{Unbuckled Driving}}
$$

The Safety Factors like "Hard Breaking" are described on the page linked above. They convert the PCF to the Safety Score using 

$$ \textrm{Safety Score} = 112.29263237 - 14.77121589 \cdot \textrm{PCF} $$

#### 6. The PCF formula looks like it could be the result of fitting a Poisson regression model to their Safety Factors and collisions data. Discuss why a Poisson Regression model would or would not be a reasonable model for this data. 

Since number of collisions is count data, predicting number of collisions based on those other variables using Poisson regression seems reasonable. Hard to know for sure without seeing the data though. Can't tell if number of collisions is roughly Poisson for a given set of predictors.

#### 7. Let's assume the PCF formula did in fact come from a Poisson Regression model. What was $\beta_{\textrm{Hard Breaking}}$, the coefficient for Hard Breaking, in that model? 

1.10385444 is $e^{\beta_{\textrm{Hard Breaking}}}$, so $\beta_{\textrm{Hard Breaking}}$ must have been 
```{r}
log(1.10385444)
```

#### 8. What is the interpretation of the term $1.16460827^{\textrm{Hard Braking}}$ in the PCF formula? 

For a one unit increase in the Hard Breaking percentage, PCF increases by 16.46%. 



### EV charging stations

The following data set is census data set from earlier in the course with new columns added that have the number of Level 2 (`lev2`) and Level 3 (`lev3`) electric vehicle charging stations for each census tract (`GEOID`). 

```{r}
dc = readRDS('data/tracts.and.census.with.EV.stations.rds')
dc = dc@data ## keep just the data frame, not the polygons
dc = dc %>% 
  
  ## Change NAs to 0s, and 
  ## since charging stations seem to come in pairs, create new columns
  ## for pairs of charging stations
  mutate(lev2 = ifelse(is.na(lev2), 0, lev2), 
         lev3 = ifelse(is.na(lev3), 0, lev3), 
         lev2pairs = round(lev2/2),
         lev3pairs = round(lev3/2)) %>%
  
  ## Keep tracts with at least one charging station, 
  ## and get rid of a couple of outliers
  filter((lev2!=0 | lev3!=0) & lev2 <= 50) 


```

#### 9. Suppose we want to quickly determine if the number of Level 2 charging stations in a census track looks roughly Poisson. (For this problem, consider all the data at once, not just the data for smaller subsets of $x$.) Let's compare the data to a Poisson distribution with the same mean as the data.  Plot the actual distribution of the data along with this Poisson distribution in a way that allows for a quick comparison of the two. Does the number pairs of Level 2 charging stations in a census tract look Poisson distributed?

```{r}
#dc = dc[dc$lev2!=0,]
mu= mean(dc$lev2, na.rm=T)
v =  var(dc$lev2, na.rm=T) 
dp = data.frame(x  =         0:20, 
                y  =   dpois(0:20, lambda = mu),
                y2 = dnbinom(0:20, mu     = mu, size=mu^2/(v-mu)))

ggplot(dc, aes(x=lev2, y=1*after_stat(density)))+
  geom_histogram(binwidth=1) + 
  geom_point(data=dp, aes(x=x, y=y), color='red')+
  geom_point(data=dp, aes(x=x, y=y2), color='blue')+
  scale_x_continuous(breaks=0:20, limits=c(NA,20))
```

Not Poisson. First the mean isn't anywhere close to the variance. 

```{r}
mu
v
```

(Of course, the mean only needs to be equal to the variance for a given value of the predictor(s) if we want to use Poisson regression, so this alone doesn't mean that Poisson would be bad. But there are other issues too.)

Also it looks like these charging companies like to install multiples of 2, 4, 8, 12, etc. If we picture in our minds that those are smoothed out, the histogram still doesn't look very Poisson and the peak isn't quite at the right spot. 

Even if we plot "pairs of Level 2 chargers", the distribution still isn't smooth. 

```{r}
mu= mean(dc$lev2pairs, na.rm=T)
v =  var(dc$lev2pairs, na.rm=T) 
dp = data.frame(x  =         0:20, 
                y  =   dpois(0:20, lambda = mu),
                y2 = dnbinom(0:20, mu     = mu, size=mu^2/(v-mu)))

ggplot(dc, 
       aes(x = lev2pairs, 
           y = 1*after_stat(density)))+
  geom_histogram(binwidth = 1) + 
  geom_point(data=dp, aes(x=x, y=y), color='red')+
  geom_point(data=dp, aes(x=x, y=y2), color='blue')+
  scale_x_continuous(breaks=0:20, limits=c(NA,20))
```


Note that in the above graphs I also plotted negative binomial with the same mean and variance as the data. That looks better, but still doesn't match the lumpiness.

Let's do Level 3 for fun, even though the question doesn't ask about it. 

```{r}
#dc = dc[dc$lev3pairs!=0,]
mu= mean(dc$lev3pairs, na.rm=T)
v =  var(dc$lev3pairs, na.rm=T) 
dp = data.frame(x  =         0:20, 
                y  =   dpois(0:20, lambda = mu),
                y2 = dnbinom(0:20, mu     = mu, size=mu^2/(v-mu)))

ggplot(dc, aes(x=lev3pairs, y=1*after_stat(density)))+
  geom_histogram(binwidth=1) + 
  geom_point(data=dp, aes(x=x, y=y), color='red')+
  geom_point(data=dp, aes(x=x, y=y2), color='blue')+
  scale_x_continuous(breaks=0:20, limits=c(NA,20))
```
```{r}
mu
v
```

Negative binomial looks better for this one. 


#### 10. Fit a Poisson Regression model for predicting Level 2 charging stations in a census tract using `house.value` and `age`. How does this model compare to the null model (the model with only an intercept term)? Considering your observations in #9, what outputs of this model are reasonable/unreasonable? 


```{r}
glm1 = glm(lev2 ~  house.value + age , 
           data=dc, 
           family='poisson')
summary(glm1)
```

The coefficients are significant, and the deviance is much lower than it is for the null model, so the model seems useful. The `house.value` coefficient is positive, as we might expect. The `age` coefficient is negative. This might be because of collinearity with `house.value`, or could be because younger drivers are more likely to own an electric vehicle, and charging stations get installed in census tracts with lower median age. 

Let's try this with log(pop) as a predictor and with log(pop) as an offset (coefficient fixed to 1), just for fun. 

```{r}
glm2 = glm(lev2 ~  house.value + age +        log(pop) , data=dc, family='poisson')
glm3 = glm(lev2 ~  house.value + age , offset=log(pop) , data=dc, family='poisson')
glm4 = glm(lev2 ~  house.value + age + offset(log(pop)), data=dc, family='poisson')

summary(glm1)$coef
summary(glm2)$coef
summary(glm3)$coef
summary(glm4)$coef

summary(glm1)$deviance
summary(glm2)$deviance
summary(glm3)$deviance
summary(glm4)$deviance
```

