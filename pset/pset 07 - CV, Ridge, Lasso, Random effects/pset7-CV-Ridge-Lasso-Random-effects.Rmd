---
title: "PSET 07 - CV, Ridge, Lasso, Random Intercepts"
author: "S&DS 361"
date: "Due 2024-04-26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS, exclude = 'select')
library(lme4)
library(arm)
library(tidyverse)
library(pubtheme)
library(corrplot)
library(glmnet)
library(pROC)

```

# Landcover

```{r}
load('data/labeled_points.Rdata')

labeled = labeled %>% 
  select(ID, landcover)

dl = labeled_train %>%
  left_join(labeled, by = 'ID') %>%
  mutate(veg = ifelse(landcover %in% c('natforest', 'orchard', 'cropland'), 
                      1, 0), 
         NDVI100 = NDVI*100, 
         NDBI100 = NDBI*100)
```

Let's compute mean band values for each location. 

```{r}
dm = dl %>%
  group_by(ID) %>%
  summarise(
    B1   = mean(B1, na.rm=T),
    B2   = mean(B2, na.rm=T),
    B3   = mean(B3, na.rm=T),
    B4   = mean(B4, na.rm=T),
    B5   = mean(B5, na.rm=T),
    B6_VCID_1 = mean(B6_VCID_1, na.rm=T),
    B6_VCID_2 = mean(B6_VCID_2, na.rm=T), ##  cor is .998 with B6_VCID_1
    B7        = mean(B7, na.rm=T),
    NDVI100   = mean(NDVI100, na.rm=T), 
    #NDBI100   = mean(NDBI100, na.rm=T), ## causes warnings with lasso
    #EVI  = mean(EVI, na.rm=T),
    landcover = unique(landcover), 
    veg = unique(veg)) %>%
  select(-ID, -landcover)
  
 
ht(dm) ## head and tail, each with 2 rows
```

## Data Exploration


Recall that there was a lot of collinearity among the mean band values at each location. 

```{r}
corr = dm %>% 
  cor(use = 'pairwise.complete.obs')
corr %>% round(2)
```


```{r}
corrplot(corr, order = 'hclust', diag=T, type = 'upper', tl.pos = 'tp')
corrplot(corr, order = 'hclust', diag=F, type = 'lower', tl.pos =  'n', 
         method='number', 
         cl.pos = 'n',
         add = T, 
         number.cex = 0.8)
```

Using `order = "hclust"` with rectangles added 

```{r}
corrplot(corr, 
         diag = F,
         order = 'hclust', 
         addrect = 4)
```

We are interested once again in modeling the probability of `veg` as a function of the mean band values. We will compare the following four models:

- logistic regression (with no regularization) using only `NDVI100` 
- logistic regression (with no regularization) using all band values
- logistic regression with ridge regularization using all band values 
- logistic regression with lasso regularization using all band values

Note that for logistic regression with regularization (3rd and 4th models), you can use the `cv.glmnet` function in the `glmnet` package as we did with linear regression, but with the argument `family = binomial` added.


## 1. Models with all of the data

Fit models with all of the data and find (in-sample) predicted probabilities for each observation. Explore the trace curves and discuss any notable observations. 



## 2. Cross-validation
Use cross-validation to make out of sample predictions for each observation and show the first 6 rows of the resulting data frame.



## 3. Log Loss 

Compute log loss for all models. Compare across models, compare in-sample vs CV log loss, and discuss any notable observations. 



# Partitioned matrices

Note that if we have partitioned matrices, or matrices written in block form, we can add and multiply matrices as usual, as if the submatrices were scalars. Consider the following simple example. Suppose the $3\times3$ matrix $A$ given in block form as

$$\underset{3\times 3}{A} = 
\begin{pmatrix}
\underset{2\times 2}{B} & \underset{2\times 1}{C} \\
\underset{1\times 2}{D} & \underset{1\times 1}{E}
\end{pmatrix}$$

where $B$ is a $2\times2$ matrix, $C$ is $2\times1$, $D$ is $1\times2$, and $E$ is $1\times1$. Suppose $E$ is a $3\times1$ vector in block form

$$\underset{3\times 1}{F} = 
\begin{pmatrix}
\underset{2\times 1}{G}  \\
\underset{1\times 1}{H} 
\end{pmatrix}$$

where $G$ is $2\times1$ and $H$ is $1\times1$.

For the following questions, you can write out your work for each question by hand and scan it, or write it up in LaTex in this document. 

## 4. Block Transpose

Show that the transpose of $A$, denoted $A^T$, is given by

$$\underset{3 \times 3}{A}^T = 
\begin{pmatrix}
\underset{2 \times 2}{B}^T & \underset{2 \times 1}{D}^T \\
\underset{1 \times 2}{C}^T & \underset{1 \times 1}{E}^T
\end{pmatrix}$$

(Write out $B = \begin{pmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix}$, $C = \begin{pmatrix} c_{11} \\ c_{21} \end{pmatrix}$, etc., and take the transpose.)




## 5. Block Multiplication

Show that

$$\underset{3 \times 1}{AF} = 
\begin{pmatrix}
\underset{2 \times 1}{BG + CH} \\
\underset{1 \times 2}{DG + EH} 
\end{pmatrix}$$

(Again, write out the matrices and multiply.)



## 6. Ridge and OLS with Augmented Data

 Using the previous two results, show ridge regression is like OLS but with augmented data. Specifically, consider

$$\underset{(n+p) \times p}{\tilde{X}} = 
\begin{pmatrix}
\underset{n \times p}{X} \\
\sqrt{\lambda} \underset{p \times p}{I} 
\end{pmatrix}$$

where $\underset{p \times p}{I}$ is a $p \times p$ identity matrix, and consider

$$\underset{(n+p) \times 1}{\tilde{y}} = 
\begin{pmatrix}
\underset{n \times 1}{y} \\
\underset{p \times 1}{0} 
\end{pmatrix}$$

where $0$ is a $p \times 1$ vector of all zeros. So $\tilde{X}$ is like $X$ that has been augmented with some extra rows, and $\tilde{y}$ is like $y$ that has been augmented with some extra zeros. Show that, for a given $\lambda$, the OLS estimates of $\tilde{y} = \tilde{X} \beta$ are the same as the ridge estimates of $y = X\beta$. (Start by writing $\tilde{X}^T\tilde{X}$ and $\tilde{X}^T \tilde{y}$.)




# NBA Games
 
Let's load and prep the data. 

```{r}
d = readRDS('data/games.rds') 
tms = read.csv('data/nba.teams.csv')

d = d %>% 
  filter(lg=='nba', season %in% 2022, season.type=='reg') %>%
  dplyr::select(date, away, home, ascore, hscore, season, gid)


da = d %>% dplyr::select(date, away, ascore, home, hscore, season, gid) %>% mutate(ha = 'away')
dh = d %>% dplyr::select(date, home, hscore, away, ascore, season, gid) %>% mutate(ha = 'home')
colnames(da) = c('date', 'team', 'score',  'opp', 'opp.score', 'season', 'gid', 'ha')
colnames(dh) = c('date', 'team', 'score',  'opp', 'opp.score', 'season', 'gid', 'ha')
d = bind_rows(da, dh) %>% 
  arrange(date, gid) %>%
  left_join(tms %>% dplyr::select(team, div), 
            by = c('team'='team')) %>%
  left_join(tms %>% dplyr::select(team, div), 
            by = c('opp'='team'), suffix=c('.team', '.opp'))
head(d)
```

Here is a function for extracting coefficients that we used in class.

```{r}
extract.ranef = function(lmer.model=NULL, lm.model=NULL){
  
  vars = names(ranef(lmer.model))
  lmer.coefs = NULL
  lm.coefs = NULL
  
  if(!is.null(lm.model)){
    lm.coefs = summary(lm.model)$coefficients %>%
      as.data.frame() %>%
      rownames_to_column(var='var') %>%
      filter(grepl(paste0(vars, collapse="|"), var)) %>%
      rename(est='Estimate',
             se ='Std. Error') %>%
      dplyr::select(var, est, se) %>%
      mutate(model='glm')
  }
  
  for (j in vars){
    
    ## lmer
    est = ranef(lmer.model)[[j]] 
    se  = se.ranef(lmer.model)[[j]] 
    colnames(est) = 'est'
    colnames(se) = 'se'
    temp = data.frame(var = rownames(est), 
                      est=est, 
                      se=se, 
                      model='glmer',
                      ranef = j)
    lmer.coefs = rbind(lmer.coefs, temp)
    
    ## lm
    if(!is.null(lm.model)){
      rows=grepl(j, lm.coefs$var)
      lm.coefs$var[rows] = lm.model$xlevels[[j]][-length(lm.model$xlevels[[j]])]
      lm.coefs$ranef[rows] = j
      
    } ## if lm.model
    
  } ## end j loop
  
  coefs = rbind(lmer.coefs, lm.coefs)
  return(coefs)
}
```

## 7. Random effects for team and opp

Previously we used `ha`, `team` and `opp` to predict `score`. Fit a linear regression model with outcome `score` and predictors `ha`, `team`, and `opp`, as well as a similar mixed effects linear regression model with both `team` and `opp` as random effects terms. Create visualizations that help you compare the coefficients obtained from the two models. Discuss any observations. 


## 8. Cross-validation

Perform cross-validation, compare the performance of the two models, discuss the results, and state which model you prefer and why.



# EV Charging Stations

Let's load and clean our charging station data. 

```{r}
dc = readRDS('data/tracts.and.census.with.EV.stations.rds')
dc = dc@data

## create a indicator for whether there is at least 
## one lev2 station in each tract
dc = dc %>% 
  mutate(l2 = ifelse(lev2 > 0 , 1,  0), 
         l2 = ifelse(is.na(l2), 0, l2)) 

```

If you have trouble answering these questions with the full data, use this line of code to take a random sample of 5,000 tracts.  Doing this will eliminate any memory issues you might have.

```{r}
dc = dc[sample(1:nrow(dc), 
               5000, 
               replace = F),] 
```


## 9. County as predictor

There are 866 counties in the US, so using county as a predictor adds 865 columns to a logistic regression model, which may not be ideal. Build a logistic regression model with `log(house.value)` and `county` as predictors, and a mixed effects logistic regression model with `log(house.value)` a random intercept for `county`. Create visualizations that help you compare the coefficients. Discuss any observations. 


## 10. Cross-validation

Perform cross-validation, compare the performance of the two models, discuss the results, and state which model you prefer and why.

